cosine.query[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
}
result_text = array()
for(i in 1:10){
result_text[11-i] = Sample[names(tail(sort(cosine.query),10)[i])]
}
result_score = array()
for(i in 1:10){
result_score[11-i] = tail(sort(cosine.query),10)[i]
}
result_score = tail(sort(cosine.query),10)[i]
final = cbind(result_score, result_text)
return(cosine.query)
}
Vew(searchEngine("economy"))
View(searchEngine("economy"))
Sample[2755]
Sample[3071]
Sample[1]
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.query = array()
for(i in 1:5000){
cosine.query[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
}
cosine.query <- cosine.query[cosine.query != 0]
result_text = array()
for(i in 1:10){
result_text[11-i] = Sample[names(tail(sort(cosine.query),10)[i])]
}
result_score = array()
for(i in 1:10){
result_score[11-i] = tail(sort(cosine.query),10)[i]
}
result_score = tail(sort(cosine.query),10)[i]
final = cbind(result_score, result_text)
return(cosine.query)
}
View(searchEngine("economy"))
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[names(tdm.matrix[1,][i])]
}
final = cbind(cosine.text, cosine.score)
# final <- final[final$cosine != 0]
return(cosine.query)
}
View(searchEngine("economy"))
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[names(tdm.matrix[1,][i])]
}
final = cbind(cosine.text, cosine.score)
# final <- final[final$cosine != 0]
return(final)
}
View(searchEngine("economy"))
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
# final <- final[final$cosine != 0]
return(final)
}
View(searchEngine("economy"))
Sample(2275)
Sample[2275]
Sample[2274]
Sample[2276]
Sample[1]
Sample[111]
Sample[1111]
Sample[2000]
Sample[2500]
Sample[2755]
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final <- final[final$cosine.score != 0]
return(final)
}
View(searchEngine("economy"))
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
# final <- final[final$cosine.score != 0]
return(final)
}
hasil = searchEngine("economy")
hasil[,1]
hasil$cosine.score
hasil#cosine.score
hasil$cosine.score
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final <- final[final$cosine.score != 0]
return(final)
}
hasil = searchEngine("economy")
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
# final <- final[final$cosine.score != 0]
return(final)
}
hasil = searchEngine("economy")
View(hasil)
hasil = hasil[hasil$cosine.score != 0]
hasil = hasil[hasil$cosine.score != 0,]
View(hasil)
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0 && final$cosine.score != NA,]
return(final)
}
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0 && !is.na(final$cosine.score)]
return(final)
}
hasil = searchEngine("economy")
View(hasil)
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0,]
return(final)
}
hasil = searchEngine("economy")
hasil = hasil[hasil$cosine.score != 0 && !is.na(hasil$cosine.text),]
View(hasil)
hasil = searchEngine("economy")
hasil = hasil[hasil$cosine.score != 0 || !is.na(hasil$cosine.text),]
View(hasil)
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0,]
return(final)
}
hasil = searchEngine("economy")
View(hasil)
hasil = hasil[hasil$cosine.score != 0 || !is.na(hasil$cosine.text),]
View(hasil)
#Mengecek file corpus
files_list <- list.files(path =
"C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey/final/en_US",
pattern = "*.txt", full.names = TRUE)
library(quanteda)
library(readtext)
library(tidytext)
library(dplyr)
library(tm)
library(SnowballC)
con1 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.blogs.txt"))
SwiftKeyBlogData = readLines(con1)
close(con1)
con2 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.news.txt"))
SwiftKeyNewsData = readLines(con2)
close(con2)
con3 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.twitter.txt"))
SwiftKeyTwitterData = readLines(con3)
close(con3)
LineCounts = c(length(SwiftKeyBlogData),length(SwiftKeyNewsData),length(SwiftKeyTwitterData))
WordCounts = c(sum(vapply(strsplit(SwiftKeyBlogData, "\\W+"), length, integer(1))),sum(vapply(strsplit(SwiftKeyNewsData, "\\W+"), length, integer(1))),sum(vapply(strsplit(SwiftKeyTwitterData, "\\W+"), length, integer(1))))
AverageWOrdCounts = c(WordCounts[1]/LineCounts[1],WordCounts[2]/LineCounts[2],WordCounts[3]/LineCounts[3])
SummaryData = data.frame(format(LineCounts,big.mark = ",", trim = TRUE), format(WordCounts,big.mark = ",", trim = TRUE), format(AverageWOrdCounts,digits = 3))
row.names(SummaryData) = c("Blogs", "News", "Twitter")
colnames(SummaryData) = c("Lines", "Words", "Avg. Words per Line")
library(gridExtra)
grid.table(SummaryData)
#Combining Datasets
SwiftKeyData = c(SwiftKeyBlogData, SwiftKeyNewsData, SwiftKeyTwitterData)
rm(SwiftKeyBlogData, SwiftKeyNewsData, SwiftKeyTwitterData)
#Sampling and Cleaning Datasets
set.seed(5432)
Sample = sample(SwiftKeyData, 5000)
N.docs = 5000
rm(SwiftKeyData)
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting=function(x) weightSMART(x,spec="ltc"), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0,]
return(final)
}
hasil = searchEngine("economy")
View(hasil)
library(tcR)
library(lsa)
#Mengecek file corpus
files_list <- list.files(path =
"C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey/final/en_US",
pattern = "*.txt", full.names = TRUE)
library(quanteda)
library(readtext) # I/O
library(dplyr)
library(tm) #term document matrix
library(SnowballC)
library(lsa) # cosine
con1 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.blogs.txt"))
SwiftKeyBlogData = readLines(con1)
close(con1)
con2 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.news.txt"))
SwiftKeyNewsData = readLines(con2)
close(con2)
con3 = file(file.path("C:/Kuliah/TKI/Pertemuan 5/Coursera-SwiftKey", "final", "en_US", "en_US.twitter.txt"))
SwiftKeyTwitterData = readLines(con3)
close(con3)
LineCounts = c(length(SwiftKeyBlogData),length(SwiftKeyNewsData),length(SwiftKeyTwitterData))
WordCounts = c(sum(vapply(strsplit(SwiftKeyBlogData, "\\W+"), length, integer(1))),sum(vapply(strsplit(SwiftKeyNewsData, "\\W+"), length, integer(1))),sum(vapply(strsplit(SwiftKeyTwitterData, "\\W+"), length, integer(1))))
AverageWOrdCounts = c(WordCounts[1]/LineCounts[1],WordCounts[2]/LineCounts[2],WordCounts[3]/LineCounts[3])
SummaryData = data.frame(format(LineCounts,big.mark = ",", trim = TRUE), format(WordCounts,big.mark = ",", trim = TRUE), format(AverageWOrdCounts,digits = 3))
row.names(SummaryData) = c("Blogs", "News", "Twitter")
colnames(SummaryData) = c("Lines", "Words", "Avg. Words per Line")
library(gridExtra)
grid.table(SummaryData)
#Combining Datasets
SwiftKeyData = c(SwiftKeyBlogData, SwiftKeyNewsData, SwiftKeyTwitterData)
rm(SwiftKeyBlogData, SwiftKeyNewsData, SwiftKeyTwitterData)
#Sampling and Cleaning Datasets
set.seed(5432)
Sample = sample(SwiftKeyData, 5000)
N.docs = 5000
rm(SwiftKeyData)
# Membuat Fungsi Query
searchEngine <- function(query){
# Record starting time to measure your search engine performance
start.time <- Sys.time()
# Membuat corpus dengan menambahkan query
my.corpus = VCorpus(VectorSource(c(Sample, query)))
# Store docs into a term document matrix where rows=terms and cols=docs
# Normalize term counts by applying TDiDF weightings
term.doc.matrix.stm <- TermDocumentMatrix(my.corpus, control=list(weighting = function(x) weightTfIdf(x, normalize = FALSE), wordLengths=c(1,Inf)))
#hitung cosine
tdm.matrix = as.matrix(term.doc.matrix.stm)
cosine.score = array()
cosine.text = array()
for(i in 1:5000){
cosine.score[names(tdm.matrix[1,][i])] = cosine(tdm.matrix[,i], tdm.matrix[,5001])
cosine.text[names(tdm.matrix[1,][i])] = Sample[i]
}
final = cbind(cosine.text, cosine.score)
final = as.data.frame(final)
final = final[final$cosine.score != 0,]
return(final)
}
hasil = searchEngine("economy")
View(hasil)
hasil
hasil = searchEngine("America President")
View(hasil)
hasil = searchEngine("car accident")
View(hasil)
hasil = searchEngine("flood")
View(hasil)
hasil = searchEngine("school")
View(hasil)
hasil = searchEngine("Airplane")
View(hasil)
hasil = searchEngine("Train")
View(hasil)
hasil = searchEngine("Bill gates")
View(hasil)
install.packages("shiny")
library(shiny)
runExample("Hello World")
runExample("01_hello")
runExample("07_widgets")
version(shiny)
package_version("shiny")
library(shiny)
# Define UI for app that draws a histogram ----
ui <- fluidPage(
# App title ----
titlePanel("Hello Shiny!"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
sliderInput(inputId = "bins",
label = "Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotOutput(outputId = "distPlot")
)
)
)
dir = "C:/Kuliah/Skripsi/Kodingan"
runApp(dir)
# Define UI for app that draws a histogram ----
ui <- fluidPage(
# App title ----
titlePanel("Hello Shiny!"),
# Sidebar layout with input and output definitions ----
sidebarLayout(
# Sidebar panel for inputs ----
sidebarPanel(
# Input: Slider for the number of bins ----
sliderInput(inputId = "bins",
label = "Number of bins:",
min = 1,
max = 50,
value = 30)
),
# Main panel for displaying outputs ----
mainPanel(
# Output: Histogram ----
plotOutput(outputId = "distPlot")
)
)
)
setwd(dir)
